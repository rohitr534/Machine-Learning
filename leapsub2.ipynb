{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":56537,"databundleVersionId":8877088,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T11:44:18.777422Z","iopub.execute_input":"2024-06-24T11:44:18.777767Z","iopub.status.idle":"2024-06-24T11:44:20.804308Z","shell.execute_reply.started":"2024-06-24T11:44:18.777740Z","shell.execute_reply":"2024-06-24T11:44:20.803217Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/leap-atmospheric-physics-ai-climsim/sample_submission.csv\n/kaggle/input/leap-atmospheric-physics-ai-climsim/test_old.csv\n/kaggle/input/leap-atmospheric-physics-ai-climsim/train.csv\n/kaggle/input/leap-atmospheric-physics-ai-climsim/test.csv\n/kaggle/input/leap-atmospheric-physics-ai-climsim/sample_submission_old.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gc\nimport os\nimport random\nimport time\nimport torch\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset , DataLoader\nfrom torchmetrics.regression import R2Score","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:20.806392Z","iopub.execute_input":"2024-06-24T11:44:20.806895Z","iopub.status.idle":"2024-06-24T11:44:31.575728Z","shell.execute_reply.started":"2024-06-24T11:44:20.806860Z","shell.execute_reply":"2024-06-24T11:44:31.574575Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/\"\nBATCH_SIZE = 12288\nMIN_STD = 1e-6\nSCHEDULER_PATIENCE = 3\nSCHEDULER_FACTOR = 10**(-0.5)\nEPOCHS = 50\nPATIENCE = 6\nPRINT_FREQ = 50","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:31.577031Z","iopub.execute_input":"2024-06-24T11:44:31.577520Z","iopub.status.idle":"2024-06-24T11:44:31.582951Z","shell.execute_reply.started":"2024-06-24T11:44:31.577492Z","shell.execute_reply":"2024-06-24T11:44:31.581944Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:31.585173Z","iopub.execute_input":"2024-06-24T11:44:31.585492Z","iopub.status.idle":"2024-06-24T11:44:31.607662Z","shell.execute_reply.started":"2024-06-24T11:44:31.585467Z","shell.execute_reply":"2024-06-24T11:44:31.606719Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def seed_everything(seed_val = 1331):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:31.608779Z","iopub.execute_input":"2024-06-24T11:44:31.609034Z","iopub.status.idle":"2024-06-24T11:44:31.617705Z","shell.execute_reply.started":"2024-06-24T11:44:31.609012Z","shell.execute_reply":"2024-06-24T11:44:31.616961Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"ts = time.time()\n\nweights = pd.read_csv(DATA_PATH + \"leap-atmospheric-physics-ai-climsim/sample_submission.csv\", nrows=1)\ndel weights['sample_id']\nweights = weights.T\nweights = weights.to_dict()[0]\ndf_train = pl.read_csv(DATA_PATH + \"leap-atmospheric-physics-ai-climsim/train.csv\", n_rows = 2_500_000)\n\n\nfor target in weights:\n    df_train = df_train.with_columns(pl.col(target).mul(weights[target]))\n\nprint(\"Time to read dataset:\",format_time(time.time()-ts),flush=True)\n\nFEAT_COLS = df_train.columns[1:557]\nTARGET_COLS = df_train.columns[557:]\n\nfor col in FEAT_COLS:\n    df_train = df_train.with_columns(pl.col(col).cast(pl.Float32))\nfor col in TARGET_COLS:\n    df_train = df_train.with_columns(pl.col(col).cast(pl.Float32))\n\n    \nx_train = df_train.select(FEAT_COLS).to_numpy()\ny_train = df_train.select(TARGET_COLS).to_numpy()\n\ndel df_train\ngc.collect()\n\nmeanx = x_train.mean(axis=0)\nstdx = np.maximum(x_train.std(axis=0),MIN_STD)\nx_train = (x_train - meanx.reshape(1,-1)) / stdx.reshape(1,-1)\n\nmeany = y_train.mean(axis=0)\nstdy = np.maximum(np.sqrt((y_train*y_train).mean(axis=0)),MIN_STD)\ny_train = (y_train - meany.reshape(1,-1)) / stdy.reshape(1,-1)\n\nprint(\"Time after processing data:\", format_time(time.time()-ts),flush = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:31.618725Z","iopub.execute_input":"2024-06-24T11:44:31.619004Z","iopub.status.idle":"2024-06-24T11:48:32.603968Z","shell.execute_reply.started":"2024-06-24T11:44:31.618973Z","shell.execute_reply":"2024-06-24T11:48:32.602807Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Time to read dataset: 0:03:25\nTime after processing data: 0:04:01\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"seed_everything()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:48:32.605531Z","iopub.execute_input":"2024-06-24T11:48:32.605843Z","iopub.status.idle":"2024-06-24T11:48:32.794365Z","shell.execute_reply.started":"2024-06-24T11:48:32.605816Z","shell.execute_reply":"2024-06-24T11:48:32.793540Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class NumpyDataset(Dataset):\n    def __init__(self, x, y):\n        assert x.shape[0] == y.shape[0], \"Features and labels must have the same number of samples\"\n        self.x = x\n        self.y = y\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, index):\n        return torch.from_numpy(self.x[index]).float().to(device), torch.from_numpy(self.y[index]).float().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:48:32.795520Z","iopub.execute_input":"2024-06-24T11:48:32.795798Z","iopub.status.idle":"2024-06-24T11:48:32.803806Z","shell.execute_reply.started":"2024-06-24T11:48:32.795773Z","shell.execute_reply":"2024-06-24T11:48:32.802760Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"dataset = NumpyDataset(x_train, y_train)\n\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:48:32.804941Z","iopub.execute_input":"2024-06-24T11:48:32.805331Z","iopub.status.idle":"2024-06-24T11:48:33.133279Z","shell.execute_reply.started":"2024-06-24T11:48:32.805302Z","shell.execute_reply":"2024-06-24T11:48:33.132229Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class FFNN(nn.Module):\n    def __init__(self, input_size, hidden_sizes, output_size):\n        super(FFNN, self).__init__()\n        \n        layers = []\n        previous_size = input_size\n        for hidden_size in hidden_sizes:\n            layers.append(nn.Linear(previous_size, hidden_size))\n            layers.append(nn.LayerNorm(hidden_size))\n            layers.append(nn.PReLU())\n            layers.append(nn.Dropout(p=0.1))\n            previous_size = hidden_size\n        \n        layers.append(nn.Linear(previous_size, output_size))\n        \n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:48:33.136617Z","iopub.execute_input":"2024-06-24T11:48:33.137009Z","iopub.status.idle":"2024-06-24T11:48:33.144080Z","shell.execute_reply.started":"2024-06-24T11:48:33.136974Z","shell.execute_reply":"2024-06-24T11:48:33.143162Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"input_size = x_train.shape[1]\noutput_size = y_train.shape[1]\nhidden_size = input_size + output_size\nmodel = FFNN(input_size, [3*hidden_size, 2*hidden_size, 2*hidden_size, 2*hidden_size, 3*hidden_size], output_size).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE)\n\nprint(\"Time after all preparations:\", format_time(time.time()-ts), flush=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:48:33.144954Z","iopub.execute_input":"2024-06-24T11:48:33.145209Z","iopub.status.idle":"2024-06-24T11:48:34.867542Z","shell.execute_reply.started":"2024-06-24T11:48:33.145164Z","shell.execute_reply":"2024-06-24T11:48:34.866649Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Time after all preparations: 0:04:03\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"best_val_loss = float('inf')\nbest_model_state = None\npatience_count = 0\nr2score = R2Score(num_outputs=len(TARGET_COLS)).to(device)\nfor epoch in range(EPOCHS):\n    print(\"\")\n    model.train()\n    total_loss = 0\n    steps = 0\n    for batch_idx, (inputs, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        steps += 1\n\n        if (batch_idx + 1) % PRINT_FREQ == 0:\n            current_lr = optimizer.param_groups[0][\"lr\"]\n            elapsed_time = format_time(time.time() - ts)\n            print(f'  Epoch: {epoch+1}',\\\n                  f'  Batch: {batch_idx + 1}/{len(train_loader)}',\\\n                  f'  Train Loss: {total_loss / steps:.4f}',\\\n                  f'  LR: {current_lr:.1e}',\\\n                  f'  Time: {elapsed_time}', flush=True)\n            total_loss = 0\n            steps = 0\n    \n\n    model.eval()\n    val_loss = 0\n    y_true = torch.tensor([], device=device)\n    all_outputs = torch.tensor([], device=device)\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            val_loss += criterion(outputs, labels).item()\n            y_true = torch.cat((y_true, labels), 0)\n            all_outputs = torch.cat((all_outputs, outputs), 0)\n    r2=0\n    r2_broken = []\n    r2_broken_names = []\n    for i in range(368):\n        r2_i = r2score(all_outputs[:, i], y_true[:, i])\n        if r2_i > 1e-6:\n            r2 += r2_i\n        else:\n            r2_broken.append(i)\n            r2_broken_names.append(FEAT_COLS[i])\n    r2 /= 368\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}')\n    print(f'{len(r2_broken)} targets were excluded during evaluation of R2 score.')\n    # print(r2_broken)\n    # print(r2_broken_names, flush=True)\n   \n    scheduler.step(avg_val_loss)\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        patience_count = 0\n        print(\"Validation loss decreased, saving new best model and resetting patience counter.\")\n    else:\n        patience_count += 1\n        print(f\"No improvement in validation loss for {patience_count} epochs.\")\n        \n    if patience_count >= PATIENCE:\n        print(\"Stopping early due to no improvement in validation loss.\")\n        break\n\ndel x_train, y_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:48:34.868560Z","iopub.execute_input":"2024-06-24T11:48:34.868941Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n  Epoch: 1   Batch: 50/184   Train Loss: 0.4890   LR: 1.0e-03   Time: 0:05:21\n  Epoch: 1   Batch: 100/184   Train Loss: 0.3538   LR: 1.0e-03   Time: 0:06:39\n  Epoch: 1   Batch: 150/184   Train Loss: 0.3435   LR: 1.0e-03   Time: 0:07:58\n\nEpoch: 1  Val Loss: 0.3103  R2 score: 0.1139\n221 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 2   Batch: 50/184   Train Loss: 0.3139   LR: 1.0e-03   Time: 0:10:38\n  Epoch: 2   Batch: 100/184   Train Loss: 0.2961   LR: 1.0e-03   Time: 0:11:57\n  Epoch: 2   Batch: 150/184   Train Loss: 0.2845   LR: 1.0e-03   Time: 0:13:18\n\nEpoch: 2  Val Loss: 0.2676  R2 score: 0.1589\n201 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 3   Batch: 50/184   Train Loss: 0.2669   LR: 1.0e-03   Time: 0:15:58\n  Epoch: 3   Batch: 100/184   Train Loss: 0.2634   LR: 1.0e-03   Time: 0:17:18\n  Epoch: 3   Batch: 150/184   Train Loss: 0.2504   LR: 1.0e-03   Time: 0:18:38\n\nEpoch: 3  Val Loss: 0.2376  R2 score: 0.1900\n200 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 4   Batch: 50/184   Train Loss: 0.2386   LR: 1.0e-03   Time: 0:21:17\n  Epoch: 4   Batch: 100/184   Train Loss: 0.2333   LR: 1.0e-03   Time: 0:22:37\n  Epoch: 4   Batch: 150/184   Train Loss: 0.2313   LR: 1.0e-03   Time: 0:23:57\n\nEpoch: 4  Val Loss: 0.2218  R2 score: 0.2109\n184 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 5   Batch: 50/184   Train Loss: 0.2234   LR: 1.0e-03   Time: 0:26:35\n  Epoch: 5   Batch: 100/184   Train Loss: 0.2180   LR: 1.0e-03   Time: 0:27:55\n  Epoch: 5   Batch: 150/184   Train Loss: 0.2168   LR: 1.0e-03   Time: 0:29:15\n\nEpoch: 5  Val Loss: 0.2091  R2 score: 0.2265\n177 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 6   Batch: 50/184   Train Loss: 0.2113   LR: 1.0e-03   Time: 0:31:54\n  Epoch: 6   Batch: 100/184   Train Loss: 0.2080   LR: 1.0e-03   Time: 0:33:14\n  Epoch: 6   Batch: 150/184   Train Loss: 0.2051   LR: 1.0e-03   Time: 0:34:33\n\nEpoch: 6  Val Loss: 0.1994  R2 score: 0.2379\n178 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 7   Batch: 50/184   Train Loss: 0.1989   LR: 1.0e-03   Time: 0:37:13\n  Epoch: 7   Batch: 100/184   Train Loss: 0.2017   LR: 1.0e-03   Time: 0:38:32\n  Epoch: 7   Batch: 150/184   Train Loss: 0.1967   LR: 1.0e-03   Time: 0:39:52\n\nEpoch: 7  Val Loss: 0.1918  R2 score: 0.2458\n179 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 8   Batch: 50/184   Train Loss: 0.1971   LR: 1.0e-03   Time: 0:42:31\n  Epoch: 8   Batch: 100/184   Train Loss: 0.1925   LR: 1.0e-03   Time: 0:43:50\n  Epoch: 8   Batch: 150/184   Train Loss: 0.1886   LR: 1.0e-03   Time: 0:45:10\n\nEpoch: 8  Val Loss: 0.1887  R2 score: 0.2535\n175 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 9   Batch: 50/184   Train Loss: 0.1913   LR: 1.0e-03   Time: 0:47:50\n  Epoch: 9   Batch: 100/184   Train Loss: 0.1878   LR: 1.0e-03   Time: 0:49:09\n  Epoch: 9   Batch: 150/184   Train Loss: 0.1907   LR: 1.0e-03   Time: 0:50:29\n\nEpoch: 9  Val Loss: 0.1811  R2 score: 0.2630\n175 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 10   Batch: 50/184   Train Loss: 0.1868   LR: 1.0e-03   Time: 0:53:09\n  Epoch: 10   Batch: 100/184   Train Loss: 0.1820   LR: 1.0e-03   Time: 0:54:29\n  Epoch: 10   Batch: 150/184   Train Loss: 0.1854   LR: 1.0e-03   Time: 0:55:49\n\nEpoch: 10  Val Loss: 0.1800  R2 score: 0.2649\n175 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 11   Batch: 50/184   Train Loss: 0.1830   LR: 1.0e-03   Time: 0:58:28\n  Epoch: 11   Batch: 100/184   Train Loss: 0.1791   LR: 1.0e-03   Time: 0:59:48\n  Epoch: 11   Batch: 150/184   Train Loss: 0.1833   LR: 1.0e-03   Time: 1:01:08\n\nEpoch: 11  Val Loss: 0.1755  R2 score: 0.2719\n175 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 12   Batch: 50/184   Train Loss: 0.1782   LR: 1.0e-03   Time: 1:03:47\n  Epoch: 12   Batch: 100/184   Train Loss: 0.1797   LR: 1.0e-03   Time: 1:05:07\n  Epoch: 12   Batch: 150/184   Train Loss: 0.1770   LR: 1.0e-03   Time: 1:06:26\n\nEpoch: 12  Val Loss: 0.1739  R2 score: 0.2750\n173 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 13   Batch: 50/184   Train Loss: 0.1768   LR: 1.0e-03   Time: 1:09:06\n  Epoch: 13   Batch: 100/184   Train Loss: 0.1745   LR: 1.0e-03   Time: 1:10:26\n  Epoch: 13   Batch: 150/184   Train Loss: 0.1778   LR: 1.0e-03   Time: 1:11:46\n\nEpoch: 13  Val Loss: 0.1706  R2 score: 0.2807\n173 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 14   Batch: 50/184   Train Loss: 0.1732   LR: 1.0e-03   Time: 1:14:25\n  Epoch: 14   Batch: 100/184   Train Loss: 0.1741   LR: 1.0e-03   Time: 1:15:45\n  Epoch: 14   Batch: 150/184   Train Loss: 0.1727   LR: 1.0e-03   Time: 1:17:05\n\nEpoch: 14  Val Loss: 0.1791  R2 score: 0.2708\n174 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 1 epochs.\n\n  Epoch: 15   Batch: 50/184   Train Loss: 0.1726   LR: 1.0e-03   Time: 1:19:45\n  Epoch: 15   Batch: 100/184   Train Loss: 0.1716   LR: 1.0e-03   Time: 1:21:05\n  Epoch: 15   Batch: 150/184   Train Loss: 0.1720   LR: 1.0e-03   Time: 1:22:24\n\nEpoch: 15  Val Loss: 0.1704  R2 score: 0.2831\n172 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 16   Batch: 50/184   Train Loss: 0.1696   LR: 1.0e-03   Time: 1:25:05\n  Epoch: 16   Batch: 100/184   Train Loss: 0.1679   LR: 1.0e-03   Time: 1:26:24\n  Epoch: 16   Batch: 150/184   Train Loss: 0.1697   LR: 1.0e-03   Time: 1:27:44\n\nEpoch: 16  Val Loss: 0.1705  R2 score: 0.2825\n173 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 1 epochs.\n\n  Epoch: 17   Batch: 50/184   Train Loss: 0.1679   LR: 1.0e-03   Time: 1:30:21\n  Epoch: 17   Batch: 100/184   Train Loss: 0.1657   LR: 1.0e-03   Time: 1:31:40\n  Epoch: 17   Batch: 150/184   Train Loss: 0.1661   LR: 1.0e-03   Time: 1:32:59\n\nEpoch: 17  Val Loss: 0.1659  R2 score: 0.2899\n172 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 18   Batch: 50/184   Train Loss: 0.1668   LR: 1.0e-03   Time: 1:35:34\n  Epoch: 18   Batch: 100/184   Train Loss: 0.1663   LR: 1.0e-03   Time: 1:36:52\n  Epoch: 18   Batch: 150/184   Train Loss: 0.1644   LR: 1.0e-03   Time: 1:38:10\n\nEpoch: 18  Val Loss: 0.1630  R2 score: 0.2943\n172 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 19   Batch: 50/184   Train Loss: 0.1636   LR: 1.0e-03   Time: 1:40:46\n  Epoch: 19   Batch: 100/184   Train Loss: 0.1620   LR: 1.0e-03   Time: 1:42:03\n  Epoch: 19   Batch: 150/184   Train Loss: 0.1629   LR: 1.0e-03   Time: 1:43:21\n\nEpoch: 19  Val Loss: 0.1661  R2 score: 0.2915\n172 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 1 epochs.\n\n  Epoch: 20   Batch: 50/184   Train Loss: 0.1635   LR: 1.0e-03   Time: 1:45:56\n  Epoch: 20   Batch: 100/184   Train Loss: 0.1612   LR: 1.0e-03   Time: 1:47:14\n  Epoch: 20   Batch: 150/184   Train Loss: 0.1603   LR: 1.0e-03   Time: 1:48:32\n\nEpoch: 20  Val Loss: 0.1798  R2 score: 0.2816\n174 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 2 epochs.\n\n  Epoch: 21   Batch: 50/184   Train Loss: 0.1672   LR: 1.0e-03   Time: 1:51:07\n  Epoch: 21   Batch: 100/184   Train Loss: 0.1593   LR: 1.0e-03   Time: 1:52:24\n  Epoch: 21   Batch: 150/184   Train Loss: 0.1591   LR: 1.0e-03   Time: 1:53:42\n\nEpoch: 21  Val Loss: 0.1610  R2 score: 0.2989\n172 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 22   Batch: 50/184   Train Loss: 0.1571   LR: 1.0e-03   Time: 1:56:16\n  Epoch: 22   Batch: 100/184   Train Loss: 0.1585   LR: 1.0e-03   Time: 1:57:33\n  Epoch: 22   Batch: 150/184   Train Loss: 0.1579   LR: 1.0e-03   Time: 1:58:51\n\nEpoch: 22  Val Loss: 0.1598  R2 score: 0.3019\n168 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 23   Batch: 50/184   Train Loss: 0.1558   LR: 1.0e-03   Time: 2:01:26\n  Epoch: 23   Batch: 100/184   Train Loss: 0.1559   LR: 1.0e-03   Time: 2:02:43\n  Epoch: 23   Batch: 150/184   Train Loss: 0.1575   LR: 1.0e-03   Time: 2:04:01\n\nEpoch: 23  Val Loss: 0.1615  R2 score: 0.3005\n169 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 1 epochs.\n\n  Epoch: 24   Batch: 50/184   Train Loss: 0.1569   LR: 1.0e-03   Time: 2:06:36\n  Epoch: 24   Batch: 100/184   Train Loss: 0.1549   LR: 1.0e-03   Time: 2:07:54\n  Epoch: 24   Batch: 150/184   Train Loss: 0.1552   LR: 1.0e-03   Time: 2:09:11\n\nEpoch: 24  Val Loss: 0.1579  R2 score: 0.3056\n166 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 25   Batch: 50/184   Train Loss: 0.1569   LR: 1.0e-03   Time: 2:11:45\n  Epoch: 25   Batch: 100/184   Train Loss: 0.1525   LR: 1.0e-03   Time: 2:13:03\n  Epoch: 25   Batch: 150/184   Train Loss: 0.1526   LR: 1.0e-03   Time: 2:14:20\n\nEpoch: 25  Val Loss: 0.1572  R2 score: 0.3079\n163 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 26   Batch: 50/184   Train Loss: 0.1524   LR: 1.0e-03   Time: 2:16:55\n  Epoch: 26   Batch: 100/184   Train Loss: 0.1542   LR: 1.0e-03   Time: 2:18:13\n  Epoch: 26   Batch: 150/184   Train Loss: 0.1508   LR: 1.0e-03   Time: 2:19:30\n\nEpoch: 26  Val Loss: 0.1592  R2 score: 0.3080\n159 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 1 epochs.\n\n  Epoch: 27   Batch: 50/184   Train Loss: 0.1531   LR: 1.0e-03   Time: 2:22:05\n  Epoch: 27   Batch: 100/184   Train Loss: 0.1517   LR: 1.0e-03   Time: 2:23:23\n  Epoch: 27   Batch: 150/184   Train Loss: 0.1518   LR: 1.0e-03   Time: 2:24:40\n\nEpoch: 27  Val Loss: 0.1565  R2 score: 0.3121\n155 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 28   Batch: 50/184   Train Loss: 0.1495   LR: 1.0e-03   Time: 2:27:14\n  Epoch: 28   Batch: 100/184   Train Loss: 0.1503   LR: 1.0e-03   Time: 2:28:32\n  Epoch: 28   Batch: 150/184   Train Loss: 0.1510   LR: 1.0e-03   Time: 2:29:50\n\nEpoch: 28  Val Loss: 0.1550  R2 score: 0.3170\n147 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 29   Batch: 50/184   Train Loss: 0.1482   LR: 1.0e-03   Time: 2:32:24\n  Epoch: 29   Batch: 100/184   Train Loss: 0.1510   LR: 1.0e-03   Time: 2:33:41\n  Epoch: 29   Batch: 150/184   Train Loss: 0.1481   LR: 1.0e-03   Time: 2:34:59\n\nEpoch: 29  Val Loss: 0.1542  R2 score: 0.3199\n140 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 30   Batch: 50/184   Train Loss: 0.1475   LR: 1.0e-03   Time: 2:37:33\n  Epoch: 30   Batch: 100/184   Train Loss: 0.1472   LR: 1.0e-03   Time: 2:38:51\n  Epoch: 30   Batch: 150/184   Train Loss: 0.1485   LR: 1.0e-03   Time: 2:40:08\n\nEpoch: 30  Val Loss: 0.1522  R2 score: 0.3254\n135 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 31   Batch: 50/184   Train Loss: 0.1459   LR: 1.0e-03   Time: 2:42:42\n  Epoch: 31   Batch: 100/184   Train Loss: 0.1462   LR: 1.0e-03   Time: 2:44:00\n  Epoch: 31   Batch: 150/184   Train Loss: 0.1463   LR: 1.0e-03   Time: 2:45:18\n\nEpoch: 31  Val Loss: 0.1531  R2 score: 0.3270\n131 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 1 epochs.\n\n  Epoch: 32   Batch: 50/184   Train Loss: 0.1448   LR: 1.0e-03   Time: 2:47:52\n  Epoch: 32   Batch: 100/184   Train Loss: 0.1450   LR: 1.0e-03   Time: 2:49:09\n  Epoch: 32   Batch: 150/184   Train Loss: 0.1460   LR: 1.0e-03   Time: 2:50:26\n\nEpoch: 32  Val Loss: 0.1527  R2 score: 0.3303\n128 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 2 epochs.\n\n  Epoch: 33   Batch: 50/184   Train Loss: 0.1452   LR: 1.0e-03   Time: 2:53:00\n  Epoch: 33   Batch: 100/184   Train Loss: 0.1452   LR: 1.0e-03   Time: 2:54:18\n  Epoch: 33   Batch: 150/184   Train Loss: 0.1449   LR: 1.0e-03   Time: 2:55:36\n\nEpoch: 33  Val Loss: 0.1515  R2 score: 0.3341\n125 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 34   Batch: 50/184   Train Loss: 0.1425   LR: 1.0e-03   Time: 2:58:10\n  Epoch: 34   Batch: 100/184   Train Loss: 0.1427   LR: 1.0e-03   Time: 2:59:27\n  Epoch: 34   Batch: 150/184   Train Loss: 0.1432   LR: 1.0e-03   Time: 3:00:44\n\nEpoch: 34  Val Loss: 0.1505  R2 score: 0.3394\n121 targets were excluded during evaluation of R2 score.\nValidation loss decreased, saving new best model and resetting patience counter.\n\n  Epoch: 35   Batch: 50/184   Train Loss: 0.1429   LR: 1.0e-03   Time: 3:03:18\n  Epoch: 35   Batch: 100/184   Train Loss: 0.1428   LR: 1.0e-03   Time: 3:04:35\n  Epoch: 35   Batch: 150/184   Train Loss: 0.1423   LR: 1.0e-03   Time: 3:05:53\n\nEpoch: 35  Val Loss: 0.1520  R2 score: 0.3390\n120 targets were excluded during evaluation of R2 score.\nNo improvement in validation loss for 1 epochs.\n\n  Epoch: 36   Batch: 50/184   Train Loss: 0.1435   LR: 1.0e-03   Time: 3:08:26\n  Epoch: 36   Batch: 100/184   Train Loss: 0.1442   LR: 1.0e-03   Time: 3:09:44\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(best_model_state)\nmodel.eval()\n\ndf_test = pl.read_csv(DATA_PATH + \"leap-atmospheric-physics-ai-climsim/test.csv\")\n\nfor col in FEAT_COLS:\n    df_test = df_test.with_columns(pl.col(col).cast(pl.Float32))\n\nx_test = df_test.select(FEAT_COLS).to_numpy()\n\nx_test = (x_test - meanx.reshape(1,-1)) / stdx.reshape(1,-1)\n\npredt = np.zeros([x_test.shape[0], output_size], dtype=np.float32)\n\ni1 = 0\nfor i in range(10000):\n    i2 = np.minimum(i1 + BATCH_SIZE, x_test.shape[0])\n    if i1 == i2:  # Break the loop if range does not change\n        break\n\n    # Convert the current slice of xt to a PyTorch tensor\n    inputs = torch.from_numpy(x_test[i1:i2, :]).float().to(device)\n\n    # No need to track gradients for inference\n    with torch.no_grad():\n        outputs = model(inputs)  # Get model predictions\n        predt[i1:i2, :] = outputs.cpu().numpy()  # Store predictions in predt\n\n    i1 = i2  # Update i1 to the end of the current batch\n\n    if i2 >= x_test.shape[0]:\n        break\n\nfor i in range(stdy.shape[0]):\n    if stdy[i] < MIN_STD * 1.1:\n        predt[:,i] = 0\n\npredt = predt * stdy.reshape(1,-1) + meany.reshape(1,-1)\n\nss = pd.read_csv(DATA_PATH + \"leap-atmospheric-physics-ai-climsim/sample_submission.csv\")\nss.iloc[:,1:] = predt\n\ndel predt\ngc.collect()\n\nuse_cols = []\nfor i in range(27):\n    use_cols.append(f\"ptend_q0002_{i}\")\n\nss2 = pd.read_csv(DATA_PATH + \"leap-atmospheric-physics-ai-climsim/sample_submission.csv\")\ndf_test = df_test.to_pandas()\nfor col in use_cols:\n    ss[col] = -df_test[col.replace(\"ptend\", \"state\")]*ss2[col]/1200.\n\ntest_polars = pl.from_pandas(ss[[\"sample_id\"]+TARGET_COLS])\ntest_polars.write_csv(\"submission.csv\")\n\nprint(\"Total time:\", format_time(time.time()-ts))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ","metadata":{}}]}